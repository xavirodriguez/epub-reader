# EPUB Narrator - Sistema HÃ­brido Local + Cloud

Sistema completo de narraciÃ³n de EPUBs con TTS local y fallback a Gemini.

## ğŸš€ Quick Start

### Requisitos
- Docker & Docker Compose
- Python 3.12+ (para desarrollo local)
- Node.js 18+ (para frontend)
- GPU recomendada (pero no obligatoria)

### InstalaciÃ³n RÃ¡pida con Docker

1. **Clonar repositorio**
```bash
git clone <repo>
cd epub-narrator-hybrid
```

2. **Configurar variables de entorno**
```bash
cp backend/.env.example backend/.env
# Editar backend/.env con tus configuraciones
```

3. **Iniciar servicios**
```bash
docker-compose up -d
```

4. **Descargar modelos (primera vez)**
```bash
docker-compose exec backend python scripts/download_models.py
```

5. **Verificar salud**
```bash
curl http://localhost:8000/api/health
```

### Desarrollo Local

#### Backend
```bash
cd backend
python3.12 -m venv venv
source venv/bin/activate
pip install -r requirements.txt

# Iniciar Ollama
ollama serve

# Iniciar FastAPI
uvicorn app.main:app --reload
```

#### Frontend
```bash
cd frontend
npm install
npm run dev
```

## ğŸ“š Arquitectura

```
Frontend (React/Vite)
    â†“
Backend (FastAPI)
    â”œâ”€â”€ TTS Manager
    â”‚   â”œâ”€â”€ Piper TTS (rÃ¡pido)
    â”‚   â”œâ”€â”€ Coqui TTS (calidad)
    â”‚   â””â”€â”€ Bark TTS (emocional)
    â”œâ”€â”€ LLM Service (Ollama)
    â”‚   â””â”€â”€ Speaker Detection
    â”œâ”€â”€ Cache (Redis)
    â””â”€â”€ Workers (Celery)
        â””â”€â”€ Chapter Export

Fallback â†’ Gemini API
```

## ğŸ¯ Endpoints Principales

### TTS
- `POST /api/tts/generate` - Generar audio
- `GET /api/tts/voices` - Listar voces
- `GET /api/tts/engines/status` - Estado engines

### Text Processing
- `POST /api/text/process` - Detectar speakers
- `POST /api/text/chunk` - Dividir texto

### Chapters
- `POST /api/chapters/export` - Exportar capÃ­tulo
- `GET /api/chapters/export/{task_id}/status` - Estado
- `GET /api/chapters/download/{task_id}` - Descargar

### Health
- `GET /api/health` - Health check completo
- `GET /api/health/ping` - Ping rÃ¡pido

## ğŸ”§ ConfiguraciÃ³n

Ver `backend/.env.example` para todas las opciones.

### Variables Clave

```bash
# TTS
DEFAULT_TTS_ENGINE=piper  # piper|coqui|bark
TTS_CACHE_ENABLED=true

# LLM
OLLAMA_MODEL=llama3.2

# Performance
MAX_CONCURRENT_TTS_JOBS=3
CHUNK_SIZE=1000
```

## ğŸ§ª Testing

```bash
# Test engines TTS
python backend/scripts/test_tts_engines.py

# Health check
curl http://localhost:8000/api/health

# Test TTS generation
curl -X POST http://localhost:8000/api/tts/generate \
  -H "Content-Type: application/json" \
  -d '{"text":"Hola mÃ³n","voice":"narradora","language":"ca"}'
```

## ğŸ“¦ Componentes

### TTS Engines
- **Piper**: RÃ¡pido, eficiente, catalÃ¡n nativo
- **Coqui**: Alta calidad, multilenguaje
- **Bark**: Emocional, experimental

### LLM
- **Ollama + Llama 3.2**: DetecciÃ³n de speakers y dialecto

### Cache
- **Redis**: Cache de audio, sesiones, resultados

### Workers
- **Celery**: Procesamiento asÃ­ncrono de capÃ­tulos

## ğŸ› Troubleshooting

### Backend no inicia
```bash
# Verificar logs
docker-compose logs backend

# Reiniciar servicios
docker-compose restart
```

### Ollama no responde
```bash
# Verificar que Ollama estÃ¡ corriendo
docker-compose ps ollama

# Pull manual del modelo
docker-compose exec ollama ollama pull llama3.2
```

### TTS falla
```bash
# Test individual de engines
docker-compose exec backend python scripts/test_tts_engines.py
```

## ğŸ“Š Monitoreo

### Flower (Celery)
http://localhost:5555

### Logs
```bash
# Backend
docker-compose logs -f backend

# Celery worker
docker-compose logs -f celery-worker

# Todos
docker-compose logs -f
```

## ğŸš€ ProducciÃ³n

1. Cambiar `ENVIRONMENT=production` en `.env`
2. Configurar SSL/TLS
3. Usar Nginx como reverse proxy
4. Configurar backups de Redis
5. Monitoreo con Prometheus/Grafana

## ğŸ“„ Licencia

MIT

## ğŸ‘¥ Contribuciones

PRs bienvenidos!
